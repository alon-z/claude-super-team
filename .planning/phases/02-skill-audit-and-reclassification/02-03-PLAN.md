---
phase: 02-skill-audit-&-reclassification
plan: 03
type: execute
wave: 3
depends_on: [02-02]
files_modified:
  - .planning/phases/02-skill-audit-&-reclassification**/02-AUDIT.md
autonomous: false

must_haves:
  truths:
    - "Every skill across all 3 plugins has been audited with findings documented"
    - "The phase-researcher agent has been audited against agent-specific frontmatter fields"
    - "Cross-skill consistency issues are identified and documented with specific recommendations"
    - "A summary table shows all 18 items with classifications at a glance"
  artifacts:
    - path: ".planning/phases/02-skill-audit-&-reclassification**/02-AUDIT.md"
      provides: "Complete audit document: 17 skills + 1 agent + consistency review + summary"
  key_links:
    - from: "02-AUDIT.md consistency section"
      to: "02-AUDIT.md all 18 entries"
      via: "Pattern analysis across entries identifying inconsistencies and opportunities"
---

<objective>
Audit the remaining 5 skills (map-codebase, marketplace-manager, skill-creator, linear-sync, github-issue-manager), the phase-researcher agent, and perform a cross-skill consistency review producing the final audit summary.

Purpose: Complete the audit across all 3 plugins and the custom agent, then synthesize cross-cutting findings that individual reviews cannot surface.
Output: Final 02-AUDIT.md with all 18 entries + consistency review + summary table ready for Phase 3 consumption
</objective>

<context>
@CAPABILITY-REFERENCE.md
@.planning/phases/02-skill-audit-&-reclassification**/02-AUDIT.md
@.planning/phases/02-skill-audit-&-reclassification**/02-RESEARCH.md
</context>

<tasks>

<task type="checkpoint:decision">
  <name>Task 1: Audit map-codebase and marketplace-utils skills</name>
  <files>
  .planning/phases/02-skill-audit-&-reclassification**/02-AUDIT.md
  plugins/claude-super-team/skills/map-codebase/SKILL.md
  plugins/marketplace-utils/skills/marketplace-manager/SKILL.md
  plugins/marketplace-utils/skills/skill-creator/SKILL.md
  </files>
  <action>
  Read the existing 02-AUDIT.md to understand patterns from Plan 01 and Plan 02 findings. Then perform the full review sequence (A through H) for each skill, one at a time.

  **Skills to review (in this order):**

  ### Skill 13: map-codebase (claude-super-team)

  Pre-audit notes from RESEARCH.md:
  - `context: fork` + opus + `disable-model-invocation: true` -- well-configured
  - Spawns 4 parallel mapper agents
  - One of only 2 skills using `context: fork`

  Key questions to investigate:
  - This is often cited as a well-configured skill -- verify that reputation
  - Does it use opus for the fork context itself, or only for the spawned agents?
  - `disable-model-invocation: true` is correct -- it should only run when explicitly invoked
  - Tool list includes Task but not AskUserQuestion -- correct for autonomous fork?
  - Supports incremental updates -- is that well-implemented?
  - No Edit tool -- does it need one for incremental merges?

  ### Skill 14: marketplace-manager (marketplace-utils)

  Pre-audit notes from RESEARCH.md:
  - haiku model but **NO `allowed-tools`** -- this is a significant gap
  - No `argument-hint`
  - References docs

  Key questions to investigate:
  - **Missing `allowed-tools` is the biggest gap in the entire audit** -- what tools does it actually use?
  - Read the full skill body to determine what tools it needs
  - haiku model -- is this appropriate for marketplace operations?
  - No argument-hint despite accepting subcommands
  - No description clarity about what operations are supported
  - This is a DIFFERENT PLUGIN -- note any convention differences from claude-super-team

  ### Skill 15: skill-creator (marketplace-utils)

  Pre-audit notes from RESEARCH.md:
  - sonnet model -- appropriate for creative/generation work
  - Has `Bash(uv run *)` specific pattern -- good practice
  - References external docs.md

  Key questions to investigate:
  - The `Bash(uv run *)` pattern is notable -- why this specific pattern? Does skill-creator scaffold Python tools?
  - sonnet model is interesting -- why not opus for a creative task?
  - Does it reference a docs.md that might be outdated?
  - Tool list includes WebFetch and WebSearch -- does it use them?
  - Is the skill well-structured for creating skills in THIS marketplace?

  **Append each completed audit entry to 02-AUDIT.md** using the established format. For marketplace-utils skills, note plugin name in the header.
  </action>
  <verify>02-AUDIT.md now contains 15 complete audit entries (skills 1-15), each with "User Decision" populated.</verify>
  <done>Map-codebase and both marketplace-utils skills audited with user-approved classifications.</done>
</task>

<task type="checkpoint:decision">
  <name>Task 2: Audit task-management skills and phase-researcher agent</name>
  <files>
  .planning/phases/02-skill-audit-&-reclassification**/02-AUDIT.md
  plugins/task-management/skills/linear-sync/SKILL.md
  plugins/task-management/skills/github-issue-manager/SKILL.md
  plugins/claude-super-team/agents/phase-researcher.md
  </files>
  <action>
  Perform the full review sequence for the 2 task-management skills, then adapt the sequence for the agent.

  **Skills to review (in this order):**

  ### Skill 16: linear-sync (task-management)

  Pre-audit notes from RESEARCH.md:
  - Has `Bash(shasum *)` specific pattern -- good practice
  - Depends on external `linear-cli` skill -- this is an external dependency

  Key questions to investigate:
  - The dependency on `linear-cli` skill is critical -- is this a skill from another plugin? An MCP tool? How is it loaded?
  - Read the full skill body to understand the delegation model
  - `Bash(shasum *)` is used for checksumming -- why?
  - Does it need `AskUserQuestion`? What interactive flows does it have?
  - No model override -- should it have one?
  - The allowed-tools list includes Edit -- why? Does it modify planning files?

  ### Skill 17: github-issue-manager (task-management)

  Pre-audit notes from RESEARCH.md:
  - sonnet model -- appropriate
  - Very specific `Bash(gh ...)` patterns -- cited as best practice example
  - Well-configured

  Key questions to investigate:
  - Verify that the `Bash(gh ...)` patterns are comprehensive -- are there gh commands it uses but doesn't declare?
  - Is this truly the best practice example for tool restrictions?
  - sonnet model -- is this the right choice for issue creation/management?
  - Does it need AskUserQuestion? The skill description suggests it creates issues programmatically
  - Could serve as the template for how other skills should restrict Bash

  ### Agent 18: phase-researcher (claude-super-team)

  **NOTE: This is an AGENT, not a skill.** Use AGENT frontmatter fields for the audit, not skill fields.

  Agent frontmatter fields to check (from CAPABILITY-REFERENCE.md Section 2):
  - `name`, `description`, `tools`, `model`, `skills`, `maxTurns`, `permissionMode`, `mcpServers`, `hooks`, `memory`

  Pre-audit notes from RESEARCH.md:
  - opus model -- appropriate for deep research
  - Preloads firecrawl skill
  - Clean agent definition

  Key questions to investigate:
  - Does it have `maxTurns` set? Should it to prevent runaway research?
  - Could it benefit from `memory: project` to learn research patterns across sessions?
  - Tool list is `Read, Write, Bash, Glob, Grep, WebSearch, WebFetch` -- is this minimal enough? Does it need Edit?
  - Does the firecrawl skill preload work correctly?
  - Is the description adequate for automatic delegation?
  - Should it have `permissionMode` set?

  **For the agent, adapt the audit entry format:**

  ```markdown
  ### Agent: phase-researcher (claude-super-team)

  **Current Frontmatter:**
  | Field | Value |
  |-------|-------|
  | name | ... |
  | description | ... |
  | tools | ... |
  | model | ... |
  | skills | ... |
  | maxTurns | {value or "not set"} |
  | permissionMode | {value or "not set"} |
  | memory | {value or "not set"} |
  | mcpServers | {value or "not set"} |
  | hooks | {value or "not set"} |

  **Behavior Summary:** {1-2 sentences}

  **Agent Frontmatter Audit:**
  | Check | Status | Finding |
  |-------|--------|---------|
  | tool restrictions | GAP / OK | {details} |
  | model selection | GAP / OK | {details} |
  | safety limits (maxTurns) | GAP / OK | {details} |
  | skill preloads | GAP / OK | {details} |
  | description quality | GAP / OK | {details} |
  | memory / learning | GAP / OK | {details} |

  **Capability Gap Analysis:**
  - {gap 1}

  **Classification Recommendation:** {Remain as Agent / Needs Feature Additions}
  **Rationale:** {why}

  **User Decision:** {user's choice}
  ```

  **Append all 3 entries to 02-AUDIT.md.**
  </action>
  <verify>02-AUDIT.md now contains 18 complete entries (17 skills + 1 agent), each with "User Decision" populated.</verify>
  <done>All 17 skills and 1 agent audited. Ready for cross-skill consistency review.</done>
</task>

<task type="auto">
  <name>Task 3: Cross-skill consistency review and final summary</name>
  <files>.planning/phases/02-skill-audit-&-reclassification**/02-AUDIT.md</files>
  <action>
  Read the complete 02-AUDIT.md with all 18 entries. Perform a cross-cutting analysis looking for systemic patterns, then append two final sections.

  **Cross-skill consistency analysis -- check for these patterns:**

  1. **Model consistency**: Are similar skills using different models without justification?
     - Compare: orchestrators (plan-phase, execute-phase, research-phase) -- should they have consistent model choices?
     - Compare: read-only status skills (progress, cst-help) -- both haiku?
     - Compare: creative/generation skills (brainstorm, skill-creator) -- consistent?

  2. **Tool restriction patterns**: Are some skills locked down while similar ones have blanket access?
     - List all skills with blanket `Bash` vs specific `Bash(pattern)` patterns
     - Identify which blanket Bash skills could feasibly restrict

  3. **Context mode consistency**: Are read-only or isolated skills missing `context: fork`?
     - Compare: progress (has fork) vs cst-help (missing fork) -- both read-only
     - Any other candidates for fork?

  4. **Invocation control**: Are skills that should NOT auto-invoke missing `disable-model-invocation: true`?
     - List current settings across all 17 skills
     - Identify misconfigurations

  5. **Description patterns**: Are descriptions following a consistent structure?
     - Compare length, specificity, and trigger phrases across skills

  6. **Argument handling**: Which skills accept arguments but lack `argument-hint`?

  7. **Cross-plugin conventions**: Do marketplace-utils and task-management skills follow the same conventions as claude-super-team?

  8. **Unused capabilities from CAPABILITY-REFERENCE.md**: Which "Documented but unused" capabilities should be adopted?
     - `disallowedTools` -- any skill that would benefit from denylist over allowlist?
     - `$ARGUMENTS[N]` indexed access -- skills that parse arguments manually
     - Dynamic context injection (`!`command``) -- status/progress skills
     - `agent` field with `context: fork` -- map-codebase or progress
     - Skill-scoped hooks -- any skill that could benefit?

  **Append to 02-AUDIT.md:**

  ```markdown
  ---

  ## Cross-Skill Consistency Review

  ### Model Selection Patterns
  {analysis}

  ### Tool Restriction Patterns
  {analysis with table of blanket vs specific Bash}

  ### Context Mode Consistency
  {analysis}

  ### Invocation Control
  {analysis with table}

  ### Description & Argument Patterns
  {analysis}

  ### Cross-Plugin Convention Gaps
  {analysis}

  ### Unused Capability Adoption Opportunities
  {analysis referencing specific CAPABILITY-REFERENCE.md items}

  ---

  ## Audit Summary

  ### Classification Results

  | # | Item | Plugin | Type | Classification | Key Gaps |
  |---|------|--------|------|---------------|----------|
  | 1 | new-project | claude-super-team | skill | {decision} | {1-2 word gap summary} |
  | 2 | create-roadmap | claude-super-team | skill | {decision} | ... |
  | ... | ... | ... | ... | ... | ... |
  | 18 | phase-researcher | claude-super-team | agent | {decision} | ... |

  ### Statistics
  - **Remain as Skill:** {count}
  - **Convert to Agent:** {count}
  - **Hybrid:** {count}
  - **Needs Feature Additions:** {count}
  - **Total gaps identified:** {count}

  ### Priority Recommendations for Phase 3
  1. {highest priority fix}
  2. {second priority}
  3. {third priority}
  ...

  ### Phase 2 Complete
  All 17 skills and 1 agent audited. Findings ready for Phase 3: Apply Audit Recommendations.
  ```
  </action>
  <verify>
  1. 02-AUDIT.md ends with "Cross-Skill Consistency Review" section containing all 8 analysis areas
  2. 02-AUDIT.md ends with "Audit Summary" containing the full 18-row classification table
  3. Statistics section counts match the actual user decisions in the entries
  4. Priority recommendations list is actionable and references specific skills/gaps
  </verify>
  <done>Complete audit document with all 18 entries, cross-skill consistency analysis, summary table, and priority recommendations for Phase 3.</done>
</task>

</tasks>

<verification>
1. 02-AUDIT.md is a single complete document with: header, 18 entries, consistency review, summary
2. All 17 skills + 1 agent have complete audit entries with user decisions
3. Review order matches LOCKED decision: new-project -> create-roadmap -> discuss-phase -> research-phase -> plan-phase -> execute-phase -> progress -> quick-plan -> phase-feedback -> brainstorm -> add-security-findings -> cst-help -> map-codebase -> marketplace-manager -> skill-creator -> linear-sync -> github-issue-manager -> phase-researcher
4. Cross-skill consistency review covers all 8 analysis dimensions
5. Summary table has correct counts matching individual entry decisions
6. Priority recommendations are specific and actionable for Phase 3
7. Phase success criteria from ROADMAP.md are met:
   - Every skill across all 3 plugins has been audited with findings documented (CHECK)
   - Each skill has a classification (CHECK)
   - Specific frontmatter/feature gaps are identified per skill (CHECK)
</verification>

<success_criteria>
The complete audit is documented in a single file ready for Phase 3 consumption. Every skill and agent has a user-approved classification. Cross-cutting consistency issues are identified. Priority recommendations guide Phase 3 execution order.
</success_criteria>

<output>
After completion, create `.planning/phases/02-skill-audit-&-reclassification**/02-03-SUMMARY.md`
</output>
